{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8273d2-bba5-4322-80dd-0196300bc054",
   "metadata": {},
   "source": [
    "# Sequential Variance-Altered MLE (SVAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b35fdc2-1129-4200-98bd-85dd62c91700",
   "metadata": {},
   "source": [
    "This is an implementation of Sequential Variance-Altered MLE (SVAM) algorithm, a robust learning algorithm for Generalized Linear Model. The algorithm refers to the paper in [arxiv](https://arxiv.org/pdf/2212.05430). This is a robust model recovery algorithm for supervised learning problems like linear regression, gamma regression, logistic regression and unsupervised learning like mean estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eef157-c01e-499e-9348-9208ea4b34e8",
   "metadata": {},
   "source": [
    "# Setup and Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff1cfa-a12b-473e-af2e-60e436f48cb6",
   "metadata": {},
   "source": [
    "It requires installation of the package scikit-learn, numpy, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cfdbd6-acfa-40ce-9881-8a39731558c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install scikit-learn\n",
    "pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c561c-2b6f-4354-ab12-3bd9a677136d",
   "metadata": {},
   "source": [
    "The codes with extension .ipynb run in Jupyter Notebook. So install Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada1baf-b096-4e9b-a6d8-f924675396f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c6f01-3006-4923-8608-477d1e2b35e2",
   "metadata": {},
   "source": [
    "This repository has four implementations of SVAM on 1) Linear LS regression 2) Logistic regression 3) mean estimation and, 4) gamma regression, respectively. The implementations are coded as Jupyter notebook files with extension .ipynb. The four implementations are in their respective folders. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ebf999-8e74-4b83-9a18-49e41b5c9953",
   "metadata": {},
   "source": [
    "# Linear Least Square Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661fbb85-baba-4c58-a7f6-3bad941c9e26",
   "metadata": {},
   "source": [
    "It learns linear regression model under constant fraction corruption, i.e. when the labels are generated as $y=w_\\ast^Tx+\\epsilon$, where noise $\\epsilon$ is generated from a zero-mean Gaussian distribution, it works.\n",
    "\n",
    "1. **Data Preparation**:\n",
    "Generate n random covariates of d-dimension from normal distribution $x\\sim\\mathcal{N}(\\mu,\\sigma)$. For a gold model $w_\\ast$ for $i=1,2,\\ldots,n$ generate $n$ corresponding labels as $y_i = < w_\\ast, x_i>$. For $\\alpha$-corruption for some constant $\\alpha$, replace $\\alpha*n$ many labels with labels generated by an adversarial model $\\tilde{w}$.\n",
    "\n",
    "2. **Run SVAM**:\n",
    "Run the main algorithm SVAM with initial (inverse) variance parameter $\\beta_0$ and increment parameter $\\eta$. SVAM computes weights and calls Weighted Linear Regression library module with the computed weights. Perform grid search to find best intialization of the two parameters\n",
    "\n",
    "3. **Plot error vs time**:\n",
    "Plot model recovery error $\\|w_\\ast-\\hat{w}_t\\|$ in L2 distance over time $t$. Compare the result of SVAM where the variance parameter gradually changes over time vs that of VAM with a fixed variance parameter for a set of fixed values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a8a61-19af-4441-8be8-85a7df8720c7",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a37546-17f0-4fdd-b036-afa24d16488b",
   "metadata": {},
   "source": [
    "For binary classification problems, solves a logistic regression where the labels are $y=\\text{sign}(w_\\ast^Tx)$ with constant fraction of label corruption\n",
    "\n",
    "1. **Data Preparation**:\n",
    "Generate n random covariates of d-dimension from normal distribution $x\\sim\\mathcal{N}(\\mu,\\sigma)$. For a gold model $w_\\ast$ for $i=1,2,\\ldots,n$ generate $n$ corresponding labels as $y_i = (1+sign(< w_\\ast, x_i>))/2$. For $\\alpha$-corruption for some constant $\\alpha$, replace $\\alpha*n$ many labels with $\\tilde{y}_i=(1+sign(< \\tilde{w}, x_i>))/2$ where $\\tilde{w}$ is an adversarial model.\n",
    "\n",
    "2. **Run SVAM**:\n",
    "Run the main algorithm SVAM with initial (inverse) variance parameter $\\beta_0$ and increment parameter $\\eta$. SVAM computes weights and calls Weighted Logistic Regression library module with the computed weights. Perform grid search to find best intialization of the two parameters\n",
    "\n",
    "3. **Plot error vs time**:\n",
    "Plot model recovery error $\\|w_\\ast-\\hat{w}_t\\|$ in L2 distance over time $t$. Compare the result of SVAM where the variance parameter gradually changes over time vs that of VAM with a fixed variance parameter for a set of fixed values. Also compare with the oracle which knows the points of corruption and thus gives the best performance benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776bdc66-5a2b-416f-8edd-298f6841e3b9",
   "metadata": {},
   "source": [
    "# Mean Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a9b24-bda8-472f-9c02-6361088cb7fd",
   "metadata": {},
   "source": [
    "In mean estimation problem $d$-dimensional covariates are generated from a normal distribution with mean $\\mu_\\ast\\in\\mathbb{R}^d$. \n",
    "\n",
    "1. **Data Preparation**:\n",
    "   For $\\alpha$-corruption, $n\\alpha$ many points are replaced by samples from a normal distribution with adversarial mean $\\tilde{\\mu}$.\n",
    "   \n",
    "2. **Run SVAM**:\n",
    "   Run SVAM with $\\beta_0$ and $\\eta$, chosen by gridsearch. Compute weights and estimate mean by weighted average.\n",
    "\n",
    "   \n",
    "3. **Plot error vs time**:\n",
    "   Plot mean estimation error $\\|\\mu_\\ast-\\hat{\\mu}_t\\|$ in L2 distance over time. Compare the SVAM result with median estimation and the oracle who knows the points that are corrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3c9e0-5ad3-4624-bef0-7d0b2ad39c30",
   "metadata": {},
   "source": [
    "# Gamma Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1a0ec-d2ac-4db5-b556-4560e4202d53",
   "metadata": {},
   "source": [
    "By Gamma regression we refer to the GLM corresponding to the Exponential Dispersion Model as a Gamma distribution which has two parameters $\\phi$ and $w_\\ast$, and the labels are given by $y=(1-\\phi)\\exp(w_\\ast^T x)$. We keep $\\phi$ constant and learn $w_\\ast$ when $\\alpha n$ labels are corrupted.\n",
    "\n",
    "1. . **Data Preparation**:\n",
    "   For $\\alpha$-corruption, $n\\alpha$ many labels are replaced by $\\tilde{y_i}=(1-\\phi)\\exp(\\tilde{w}^T x_i)$.\n",
    "   \n",
    "2. **Run SVAM**:\n",
    "   Run SVAM with $\\beta_0$ and $\\eta$, chosen by gridsearch. Compute weights and take gradient step on weighted likelihood.\n",
    "\n",
    "   \n",
    "3. **Plot error vs time**:\n",
    "   Plot mean estimation error $\\|w_\\ast-\\hat{w}_t\\|$ L2-distance over time. Compare the SVAM result with MLE and fixed variance VAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
